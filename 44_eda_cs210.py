# -*- coding: utf-8 -*-
"""44_EDA_CS210.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uoJdTKK19wKUVAB5OQoZoSquOJXHp2To

#Group ID and Project Members

**Group ID: 44 
  Irmak Çoban 26839
  Arda Göktaş 26708
  Ömer Faruk Altıntel 26327
  Mustafa Sergen Haysal 25431
  **

# Introduction: SPOTIFY SONG & GENRE ANALYSIS, POPULARITY PREDICTION and BUILDING A SIMPLE RECOMMENDATION SYSTEM

As the dataset provides many songs with pre-calculated features along with the genre information, there is a great potential to analyze how these features change with different genres. As the dataset spans over a long time, how the features of genres change over time can be analyzed at both Exploratory Data Analysis and Statistical Analysis & Hypothesis Testing sections.

# Utilized Datasets

Link part of the data sets used: https: //www.kaggle.com/yamaerenay/spotify-dataset-19212020-160k-tracks

There were five issues observed. Firstly, the data was visualized and analyzed in detail .In the second part, the change of properties in the data over the years has been examined. Then, the most famous artists and songs in the data were examined. Afterwards, it was examined in more detail how songs of different genres have changed their characteristics over time. In the last part, the different characteristics of the subgenres of these different genres were examined, and their changes over the years were prepared with graphics. Technically, line plot and histogram were used in the graphics because they are more suitable for the project.

# MACHINE LEARNING

## Preperation and Modeling Our Data

Used Libraries Section
"""

from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from yellowbrick.features import JointPlotVisualizer
from sklearn.preprocessing import StandardScaler
from yellowbrick.target import FeatureCorrelation
from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LinearRegression
from sklearn import metrics
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.linear_model import LogisticRegression

"""Used Dataframe Section"""

df.head(5)

"""The Preparation of the Data and How the Features Relate to the Popularity Index for the Song."""

feature_names = ['acousticness', 'danceability', 'energy', 'instrumentalness',
       'liveness', 'loudness', 'speechiness', 'tempo', 'valence','duration_ms','explicit','year']

X, y = df[feature_names], df['popularity']

features = np.array(feature_names)

visualizer = FeatureCorrelation(labels=features)

plt.rcParams['figure.figsize']=(20,20)
plt.title("features correlation for index of popularity of songs",fontsize=20)
plt.yticks(fontsize=20)
plt.xticks(fontsize=20)

visualizer.fit(X, y)        # Fit the data to the visualizer
visualizer.draw()

"""## Prediction of Song Popularity with Various Machine Learning Models

Partition Proportions

We used 80-10-10 data proportion.
80 for train set and 10-10 for valid and test set.

**Train Dataset:** A sample of the actual data to be used for training your model. Your model will be learning from this set.


**Validation Dataset:** A sample from the actual data to be utilized for hyperparameter fine-tunning.

**Test Dataset:** The sample to evaluate your model's performance.

---
"""

# 80% for training and 20% for testing-validation
train_X, X_remaining, train_y, y_remaining = train_test_split(X, y, test_size=0.20, random_state=42)
# 10% validation, 10% test
test_X, X_val, test_y, y_val = train_test_split(X_remaining, y_remaining, test_size=0.50, random_state=42)

"""Evaluation Metrics

*MSE (mean squared error):*  $\frac{1}{N}\sum_i \, (y_{true_i} - y_{pred_i})^2 \,$


*RMSE (root mean squared error):*  $\sqrt{\frac{1}{N}\sum_i \, (y_{true_i} - y_{pred_i})^2}$


*MAE (mean absolute error):* $\sum_i \, |y_{true_i} - y_{pred_i}| \,$

Since we are measuring the errors, we want these metrics to be as minimum as possible for a good performing model.

---

**Linear Regression**

A statistical model that that examines the linear relationship between two (simple linear regression) or more (multiple linear regression) variables.
"""

model_linear = LinearRegression()  
model_linear.fit(train_X, train_y)

y_pred = model_linear.predict(test_X)

mse = mean_squared_error(test_y, y_pred)
mae = mean_absolute_error(test_y, y_pred)
rmse = np.sqrt(mse)


print("mse: {}".format(mse)) # MSE (mean squared error)
print("mae: {}".format(mae)) # MAE (mean absolute error)
print("rmse: {}".format(rmse)) # RMSE (root mean squared error)

"""**Logistic Regression**

Let us try to understand logistic regression by considering a logistic model with given parameters, then seeing how the coefficients can be estimated from data. Consider a model with two predictors, 
x
1
x_{1} and 
x
2
x_{2}, and one binary (Bernoulli) response variable 
Y
Y, which we denote 
p
=
P
(
Y
=
1
)
{\displaystyle p=P(Y=1)}. We assume a linear relationship between the predictor variables and the log-odds (also called logit) of the event that 
Y
=
1
Y=1. This linear relationship can be written in the following mathematical form (where ℓ is the log-odds, 
b
b is the base of the logarithm, and 
β
i
\beta _{i} are parameters of the model):
"""

from sklearn.linear_model import LogisticRegression
LR_Model = LogisticRegression()
LR_Model.fit(train_X , train_y)
LR_Predict = LR_Model.predict(test_X) #1 minute 29 second / run time without hyper-parameter

mse = mean_squared_error(test_y, LR_Predict)
mae = mean_absolute_error(test_y, LR_Predict)
rmse = np.sqrt(mse)
print("mse: {}".format(mse)) # MSE (mean squared error)
print("mae: {}".format(mae)) # MAE (mean absolute error)
print("rmse: {}".format(rmse)) # RMSE (root mean squared error)

"""**Random Forest Regressor**

Random Forest is a supervised machine learning algorithm which can be used for both classification and regression problems. We run many decision trees independently and aggregate the outputs with a voting based approach.
"""

model_randomforest = RandomForestRegressor()
model_randomforest.fit(train_X, train_y) # 3 minute  21 second / run time without hyper-parameter

preds = model_randomforest.predict(test_X)
mse = mean_squared_error(test_y, preds)
mae = mean_absolute_error(test_y, preds)
rmse = np.sqrt(mse)
print("mse: {}".format(mse)) # MSE (mean squared error)
print("mae: {}".format(mae)) # MAE (mean absolute error)
print("rmse: {}".format(rmse)) # RMSE (root mean squared error)

"""**Decision Tree Regressor **

Decision trees are *supervised* machine learning algorithms which can be used for both *classification* and *regression* problems. The model we create is based on a *tree*, in the sense that we divide the data into subsets, i.e. *branches*, over and over again to generate a set of rules.
"""

model_Decision = DecisionTreeRegressor()
model_Decision.fit(train_X, train_y) # 3 second / run time without hyper-parameter

pred_dec = model_Decision.predict(test_X)
mse = mean_squared_error(test_y, pred_dec)
mae = mean_absolute_error(test_y, pred_dec)
rmse = np.sqrt(mse)
print("mse: {}".format(mse)) # MSE (mean squared error)
print("mae: {}".format(mae)) # MAE (mean absolute error)
print("rmse: {}".format(rmse))# RMSE (root mean squared error)

"""**Kneighbors Regressor**

kNN is a simple, yet a powerful machine learning algorithm. The main idea behind kNN is to check the surroundings of the given instance. It's a supervised algorithm and can be used in both classification and regression. It simply calculates the distance of a new data point to all other training data points. It then selects the K-nearest data points, where K can be any integer. Finally, it assigns the data point to the class to which the majority of the K data points belong.
"""

from sklearn.neighbors import KNeighborsRegressor
Model_KNN = KNeighborsRegressor()
Model_KNN.fit(train_X,train_y)

pred_dec = Model_KNN.predict(test_X)
mse = mean_squared_error(test_y, pred_dec)
mae = mean_absolute_error(test_y, pred_dec)
rmse = np.sqrt(mse)
print("mse: {}".format(mse)) # MSE (mean squared error)
print("mae: {}".format(mae)) # MAE (mean absolute error)
print("rmse: {}".format(rmse))# RMSE (root mean squared error)

"""**In Summary;**

1.   Linear Regressor (mae= 13.45)

2.   Logistic Regressor (mae = 25.51 )

3.   Decision Tree Regressor (mae = 10.29)

4.   Random Forest Regressor (mae = 8.13 )

5.   Kneighbors Regressor (mae = 13.85)

### Conclusion

we want to value of mean absolute error to minimum value for correctness of our model we test this mae with test data set and we examine minimum mae without hyper-parameter is 8.13 in random forest regressor.

## Efforts on hyper-parameter tuning to increase the performance of models

We can see parameters of models with get_params function

**Logistic Regressor Hyper-paramater**
"""

LR_Model.get_params()

LR_Model = LogisticRegression(max_iter=5,intercept_scaling=1,tol=0.0001,random_state=0,n_jobs=5)
LR_Model.fit(train_X , train_y)
LR_Predict = LR_Model.predict(test_X) #9second run time with hyper-parameters

mae = mean_absolute_error(test_y, LR_Predict)
print("mae: {}".format(mae)) # MAE (mean absolute error)

"""As easily can see that we found mae 25.50 with hyperparameter in logistic regression. it is the same result without hyper-parameter but usage of ram is minumum comparison to without hyper-parameter

**Decision Tree Regressor **
"""

model_Decision.get_params()

model_Decision.get_depth()

dec_tree2 = DecisionTreeRegressor(max_depth=6, max_features=None, 
                                  min_samples_leaf=1, min_samples_split=2, random_state=1200)
dec_tree2.fit(train_X, train_y)
val_preds2 = dec_tree2.predict(test_X)
val_mae2 = mean_absolute_error(test_y, val_preds2)
print("mae: {}".format(val_mae2))

"""In **Decision Tree Model** without hyper-parameter is mae is 10.29 but we can decrease value of mean absolute error with hyper-parameters

**Random Forest Regressor**
"""

model_randomforest.get_params()

random_forest = RandomForestRegressor(max_depth=9, max_features=None,min_samples_leaf=1, min_samples_split=2, random_state=1200,n_estimators=100,n_jobs=5)
random_forest.fit(train_X, train_y)
val_preds = random_forest.predict(test_X)
val_mae = mean_absolute_error(test_y, val_preds)
print("mae: {}".format(val_mae)) # 1dk 4sn / runtime with hyperparameter

"""For **Random Forest Regressor** when we use the hyper-parameters its usage of ram is decreasing but its value of mean absolute error is increasing.

**Kneigbors regressor**
"""

Model_KNN.get_params()

Model_KNN = KNeighborsRegressor(leaf_size=30,n_neighbors=20,n_jobs=12,metric='minkowski')
Model_KNN.fit(train_X,train_y)

pred_dec = Model_KNN.predict(test_X)

mae = mean_absolute_error(test_y, pred_dec)

print("mae: {}".format(mae)) # MAE (mean absolute error)

"""## Creating a simple song recommendation system (The system can work as accepting a list of songs as input and returns a selected number of songs similar to the ones given in the input) using similarity metrics and Nearest Neighbors methods

"""

df_new = df.drop(['artists','duration_ms','explicit','key','mode','release_date','name','popularity','year'],axis=1)
df_new.head(5)

df_new.index = df_new['id']
df_new = df_new.drop(['id'],axis=1)

df_new.head()

from sklearn.neighbors import NearestNeighbors
from scipy.sparse import csr_matrix

model_knn = NearestNeighbors(algorithm='kd_tree',n_neighbors=20)
mat_songs = csr_matrix(df_new.values)

model_knn.fit(mat_songs)

def recommend(idx, model, number_of_recommendations=5):
    query = df_new.loc[idx].to_numpy().reshape(1,-1)
    print('Searching for recommendations...')
    distances, indices = model.kneighbors(query,n_neighbors = number_of_recommendations)
    
    for i in indices:
        print(df[['name','artists']].loc[i].where(df['id']!=idx).dropna())

name = input('Enter song title: ')
print('Search results: ')
print(df[['artists','name']].where(df['name'] == name).dropna())

ind = int(input('Enter the index value of the required song: '))
idx = df['id'].loc[ind]

song = df['name'].loc[ind]
artists = df['artists'].loc[ind]

print('Song selected is ', song, 'by', artists)

nor = int(input('Enter number of recommendations: '))

recommend(idx, model_knn, nor+1)

name = input('Enter song title: ')
print('Search results: ')
print(df[['artists','name']].where(df['name'] == name).dropna())

ind = int(input('Enter the index value of the required song: '))
idx = df['id'].loc[ind]

song = df['name'].loc[ind]
artists = df['artists'].loc[ind]

print('Song selected is ', song, 'by', artists)

nor = int(input('Enter number of recommendations: '))
recommend(idx, model_knn, nor+1)

"""**Basic song reccomendation system with index and name of the songs**

## Song clustering based on available features and trying to relate them with existing genres
"""

df5.head(5)

x = df5[['acousticness', 'speechiness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness', 'tempo', 'valence']]

y = df5[['popularity']]

info = df5[['name', 'artists', 'genres']]

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
x_ss = ss.fit_transform(x)

# Create a PC space with 2 components only
pca = PCA(n_components=2)

pc = pca.fit_transform(x_ss)

Sum_of_squared_distances = []
K = range(1,15)
for k in K:
    km = KMeans(n_clusters=k)
    km = km.fit(pc)
    Sum_of_squared_distances.append(km.inertia_)

plt.plot(K, Sum_of_squared_distances, 'bx-')
plt.xlabel('k')
plt.ylabel('Sum_of_squared_distances')
plt.title('Elbow Method For Optimal k')
plt.show()

"""We found that the optimal cluster number is 5 with **Elbow Method**."""

km = KMeans(n_clusters=5, init='k-means++', n_init=10, max_iter=300, random_state=1206)
# Fit and predict with 5 clusters using Principal Components
pred = km.fit_predict(pc)

# Create a Dataframe for PC with target variable
df = pd.DataFrame(data = pc, columns = ['PC1', 'PC2'])

df['Clusters']=pd.Series(pred)

df = pd.concat([df, y, info], axis = 1)

df.head(10)

"""**Kmeans with 3D**"""

# Plot using seaborn our clusterization
plt.figure(figsize=(20,10));
sns.scatterplot(x="PC1", y="PC2", hue="Clusters", data=df, palette=['green','blue','red'], s= 50);

# Create a PC space with 3 components only
pca = PCA(n_components=3)

pc = pca.fit_transform(x_ss)

km = KMeans(n_clusters=11, init='k-means++')

pred = km.fit_predict(pc)

df = pd.DataFrame(data = pc, columns = ['PC1', 'PC2','PC3'])

df['Clusters']=pd.Series(pred)

df = pd.concat([df, y, info], axis = 1)

df

import plotly.express as px
px.scatter_3d(df, x='PC1', y='PC2', z='PC3', color='Clusters')

"""# HYPOTHESIS TESTING

## Statistical Tests to Check How (or if) Features Contribute to Popularity of Songs.

### Introduction: In this section, our aim is to observe how the characteristics of the songs affect the popularity of the songs.

In this part, we use the data-csv.
"""

fig, axes = plt.subplots(1,2)
axes[0].set_title("Dist. of  acousticness")
axes[0].hist(df['acousticness'], alpha=0.3, density=True)
axes[0].axvline(df['acousticness'].mean(), 0, 1, c="r")
axes[1].set_title("Distribution of valence")
axes[1].hist(df['valence'], alpha=0.3, density=True)
axes[1].axvline(df['valence'].mean(), 0, 1, c="r")


plt.tight_layout()
plt.show()

"""Mean = Sum of All Data Points / Number of Data Points."""

fig, ax = plt.subplots(figsize=(15, 6))
ax1_data =  df.groupby('acousticness')['popularity'].mean().to_frame().reset_index()
ax = sns.scatterplot(x=ax1_data['acousticness'], y=ax1_data['popularity'], color='red', ax=ax)
ax.set_title('Acousticness vs. Mean Popularity')
ax.set_ylabel('Mean Popularity', fontsize=12)
plt.tight_layout()
plt.show()

fig, axes = plt.subplots(figsize=(15, 6))
ax1_data =  df.groupby('valence')['popularity'].mean().to_frame().reset_index()
axes = sns.scatterplot(x=ax1_data['valence'], y=ax1_data['popularity'], color='DARKGOLDENROD', axes=axes)
axes.set_title('valence vs. Mean Popularity')
axes.set_ylabel('Mean Popularity', fontsize=12)
plt.tight_layout()
plt.show()

"""As can be clearly seen in the examples above, the mean of popularity varies according to the characteristics of the songs.
Now, we need to apply a Hypothesis test to clear and prove this relationships.

### Hypothesis Test
Hypothesis Test: We want to test whether there is correlation between popularity of songs and their features or above graph is due to chance.

**Null Hypothesis ($H_0$)**: There is no correlation between 'features' and popularity of songs. For all case correlation is equal and 0.

$ H_0: \mu_{ci_1} = \mu_{ci_2} = \mu_{ci_3}$

**Alternative Hypothesis ($H_A$)**: There is correlation between 'features' and popularity of songs.For all case correlation is different.

$ H_A:$ Means $\mu_{ci_1}, \mu_{ci_2}, \mu_{ci_3}$ are not same.

**Significance Level**: As most of hypothesis tests assume significance level as `0.05`, we are setting it as `0.05` for our test too.
"""

df['popularity'].corr(df['acousticness'])

resultp = scipy.stats.linregress(df['popularity'], df['acousticness'])

resultp.pvalue

resultp.stderr

df['popularity'].corr(df['valence'])

result1p = scipy.stats.linregress(df['popularity'], df['valence'])
result1p.pvalue

result1p.stderr

"""We can use this method for find p values and correlations. Also, we can use following method to find p values.

"""

f_stats, p_values = f_oneway(df['popularity'].values, df['acousticness'].values)
p_values

f1_stats, p1_values = f_oneway(df['popularity'].values, df['valence'].values)
p1_values

"""### OUTRO

In the last method, we accepted the value of p-value as 0 because it is very close to 0. However, we can see the p-values of popularity and valence in the first method, and this value is very small. Therefore, this value is small in our significance value (0.05).

As p-value we obtained that is smaller than the threshold significance level 0.05, we can conclude that popularity of songs and features have a relationship. Here, we reject the null hypothesis. Also, we find that correlation of acousticness and popularity has a negatif and correlation of valence / popularity has a positive.

## Statistical Tests to Check If Significant Differences Exist Between Different Eras (like comparing features of 80s and 90s hip-hop)

### Introduction: We Need to Clear and Prepare Data for the Hypothesis Testing.
"""

justRock.head()

justrock8s=justRock.copy()
justrock8s=justrock8s[justrock8s['year']>= 1980]
justrock8s=justrock8s[justrock8s['year'] < 1990]
target1= 0
justrock8s['target'] = target1
justrock8s

justrock8s.describe()

justRock9s=justRock.copy()
justRock9s=justRock9s[justRock9s['year']>=1990]
justRock9s=justRock9s[justRock9s['year']< 2000]
target = 1
justRock9s['target'] = target
justRock9s

justRock9s.describe()

vertical_stack = pd.concat([justrock8s, justRock9s], axis=0)
vertical_stack

"""### Hypothesis Testing

Sample Hypothesis Test

Hypothesis Test: We want to test whether there is significant differences in terms of eras of rock for different features.

**Null Hypothesis ($H_0$)**: Means of ` eras of rock` samples for all target values are same (e.g. $ci_1$ denotes 1st eras of rock sample).

$ H_0: \mu_{ci_1} = \mu_{ci_2} = \mu_{ci_3}$

**Alternative Hypothesis ($H_A$)**: Means of ` eras of rock` samples for all target values are different.

$ H_A:$ Means $\mu_{ci_1}, \mu_{ci_2}, \mu_{ci_3}$ are not same.

**Significance Level**: As most of hypothesis tests assume significance level as `0.05`, we are setting it as `0.05` for our test too.
"""

plt.figure(figsize=(16, 10))
sns.set(style="whitegrid")
columns = ["acousticness","danceability","instrumentalness","liveness","valence"]
for col in columns:
    x = justrock8s.groupby("year")[col].mean()
    ax= sns.lineplot(x=x.index,y=x,label=col)
ax.set_title('Audio characteristics of songs for selected features over year')
ax.set_ylabel('mean')
ax.set_xlabel('Year')

plt.figure(figsize=(16, 10))
sns.set(style="whitegrid")
columns = ["acousticness","danceability","instrumentalness","liveness","valence"]
for col in columns:
    x = justRock9s.groupby("year")[col].mean()
    ax= sns.lineplot(x=x.index,y=x,label=col)
ax.set_title('Audio characteristics of songs for selected features over year')
ax.set_ylabel('mean')
ax.set_xlabel('Year')

sample_1, sample_2 = [vertical_stack[vertical_stack['target'] == i] for i in vertical_stack.target.unique()]

fig, ax = plt.subplots(1, 3, figsize=(14,5))
        
sample_1['acousticness'].plot(kind="hist", ax=ax[0], bins=20, label="completed", color="c", density=True)
ax[0].set_title("Target: 0")

sample_2['acousticness'].plot(kind="hist", ax=ax[1], bins=20, label="none", color="m", density=True)
ax[1].set_title("Target: 1")


sns.kdeplot(sample_1['acousticness'], shade=True, label="Target: 0", ax=ax[2], color="c")
sns.kdeplot(sample_2['acousticness'], shade=True, label="Target: 1", ax=ax[2], color="m")

ax[2].set_title("Comparison with KDE")

plt.suptitle("Acousticness Distrubition")
# To avoid suptitle and titles of ax titles colliding
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""P-value for Acousticness in 80s-90s Rock"""

f3_stats, p3_values = f_oneway(sample_1['acousticness'].values, sample_2['acousticness'].values)
p3_values

fig, ax = plt.subplots(1, 3, figsize=(14,5))
        
sample_1['danceability'].plot(kind="hist", ax=ax[0], bins=20, label="completed", color="c", density=True)
ax[0].set_title("Target: 0")

sample_2['danceability'].plot(kind="hist", ax=ax[1], bins=20, label="none", color="m", density=True)
ax[1].set_title("Target: 1")


sns.kdeplot(sample_1['danceability'], shade=True, label="Target: 0", ax=ax[2], color="c")
sns.kdeplot(sample_2['danceability'], shade=True, label="Target: 1", ax=ax[2], color="m")

ax[2].set_title("Comparison with KDE")

plt.suptitle("Danceability distrubition")
# To avoid suptitle and titles of ax titles colliding
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""P-value for Danceability in 80s-90s Rock"""

f4_stats, p4_values = f_oneway(sample_1['danceability'].values, sample_2['danceability'].values)
p4_values

"""### Conclusion

As seen above, The p-values of the rock type in terms of acousticness and danceability are greater than the signifiance value(0.05). Therefore, we have to accept Null Hypothesis. Consequently, the mean of acousticness and danceability in rock from 80-90 is the same.

## Statistical Tests on Sub-genres (investigating if there are Significant Statistical Differences in Features of Different Sub-genres of the Same Parent Genre, eg. Comparing Features of British Rock and Anadolu Rock Songs).

### Introduction: In this section, we use dataofrock data and dataofrap that we created before and we use these data for concating new dataframes.
"""

dataOfRock.head()

dataOfRock['genres'].value_counts()[:10]

art_rock=dataOfRock[dataOfRock['genres']=='art rock'] 
dance_rock=dataOfRock[dataOfRock['genres']=='dance rock']
vertical_stack_2 = pd.concat([art_rock, dance_rock], axis=0)
vertical_stack_2

"""### Hypothesis Testing For Sub-genres of Rock."""

sample_3, sample_4 = [vertical_stack_2[vertical_stack_2['genres'] == i] for i in vertical_stack_2.genres.unique()]

fig, ax = plt.subplots(1, 3, figsize=(14,5))
        
sample_3['danceability'].plot(kind="hist", ax=ax[0], bins=20, label="completed", color="c", density=True)
ax[0].set_title("genre: art rock")

sample_4['danceability'].plot(kind="hist", ax=ax[1], bins=20, label="none", color="m", density=True)
ax[1].set_title("genre: dance rock")


sns.kdeplot(sample_3['danceability'], shade=True, label="genre: art rock", ax=ax[2], color="c")
sns.kdeplot(sample_4['danceability'], shade=True, label="Target: dance rock", ax=ax[2], color="m")

ax[2].set_title("Comparison with KDE")

plt.suptitle("Danceability distrubition")
# To avoid suptitle and titles of ax titles colliding
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""P-value for Danceability in Art rock and Dance Rock"""

f4_stats, p4_values = f_oneway(sample_3['danceability'].values, sample_4['danceability'].values)
p4_values

fig, ax = plt.subplots(1, 3, figsize=(14,5))
        
sample_3['acousticness'].plot(kind="hist", ax=ax[0], bins=20, label="completed", color="c", density=True)
ax[0].set_title("genre: art rock")

sample_4['acousticness'].plot(kind="hist", ax=ax[1], bins=20, label="none", color="m", density=True)
ax[1].set_title("genre: dance rock")


sns.kdeplot(sample_3['acousticness'], shade=True, label="genre: art rock", ax=ax[2], color="c")
sns.kdeplot(sample_4['acousticness'], shade=True, label="Target: dance rock", ax=ax[2], color="m")

ax[2].set_title("Comparison with KDE")

plt.suptitle("acousticness distrubition")
# To avoid suptitle and titles of ax titles colliding
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""P-value for Acousticness in Art rock and Dance Rock"""

f5_stats, p5_values = f_oneway(sample_3['acousticness'].values, sample_4['acousticness'].values)
p5_values

"""### Conclusion for Hypothesis Testing of Sub-genres of Rock.

In this section, the 2 subgenres of rock, dance rock and art rock, were compared in terms of danceability and acousticness. Their p-values are less than their signifiance value(0.05). Hence, we cannot accept Null Hypothesis.

### Hypothesis Testing for Sub-genres of Rap.
"""

dataOfRap['genres'].value_counts() [0:10]

chicago_rap=dataOfRap[dataOfRap['genres']=='chicago rap'] 
melodic_rap=dataOfRap[dataOfRap['genres']=='melodic rap']
vertical_stack_3 = pd.concat([chicago_rap, melodic_rap], axis=0)
vertical_stack_3

sample_5, sample_6 = [vertical_stack_3[vertical_stack_3['genres'] == i] for i in vertical_stack_3.genres.unique()]

ig, ax = plt.subplots(1, 3, figsize=(14,5))
        
sample_5['danceability'].plot(kind="hist", ax=ax[0], bins=20, label="completed", color="c", density=True)
ax[0].set_title("genre: chicago rap")

sample_6['danceability'].plot(kind="hist", ax=ax[1], bins=20, label="none", color="m", density=True)
ax[1].set_title("genre: melodic rap")


sns.kdeplot(sample_5['danceability'], shade=True, label="genre: chicago rap", ax=ax[2], color="c")
sns.kdeplot(sample_6['danceability'], shade=True, label="Target: melodic rap", ax=ax[2], color="m")

ax[2].set_title("Comparison with KDE")

plt.suptitle("danceability distrubition")
# To avoid suptitle and titles of ax titles colliding
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""P-value for Danceability in Chicago Rap and Melodic Rap"""

f6_stats, p6_values = f_oneway(sample_5['danceability'].values, sample_6['danceability'].values)
p6_values

ig, ax = plt.subplots(1, 3, figsize=(14,5))
        
sample_5['liveness'].plot(kind="hist", ax=ax[0], bins=20, label="completed", color="c", density=True)
ax[0].set_title("genre: chicago rap")

sample_6['liveness'].plot(kind="hist", ax=ax[1], bins=20, label="none", color="m", density=True)
ax[1].set_title("genre: melodic rap")


sns.kdeplot(sample_5['liveness'], shade=True, label="genre: chicago rap", ax=ax[2], color="c")
sns.kdeplot(sample_6['liveness'], shade=True, label="Target: melodic rap", ax=ax[2], color="m")

ax[2].set_title("Comparison with KDE")

plt.suptitle("liveness distrubition")
# To avoid suptitle and titles of ax titles colliding
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""P-value for liveness in Chicago Rap and Melodic Rap"""

f7_stats, p7_values = f_oneway(sample_5['liveness'].values, sample_6['liveness'].values)
p7_values

"""### Conclusion for Hypothesis Testing of Sub-genres of Rap.

In this section, chicago rap and melodic rap, two sub-types of rap, are examined in terms of danceability and liveness. In the danceability part, we can accept Null Hypothesis since p-value is greater than the signifiance value. However, the liveness part is the opposite of the danceability part. P-value is less than signifiance value(0.05). Thus, we cannot accept Null Hypothesis.

#EXPLOTARY DATA ANALYSIS

*   Visualizations descriptive statistics of the dataset; visual explanations of features & sharing distributions
*   Example visualizations of aggregated forms based on features
*   Analysis of the most popular artists and songs.
*   Temporal analysis of feature
*   Comparison of features of sub-genres for selected parent genres over time periods
"""

#from google.colab import drive
#drive.mount('./drive', force_remount=True)

#path_prefix = "./drive/My Drive/"

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from os.path import join
import warnings
import seaborn as sns
from sklearn import datasets
import numpy as np
import scipy.stats
from scipy.stats import f_oneway
# %matplotlib inline



#read the dataset and display the first five rows of it
warnings.filterwarnings('ignore')
df=pd.read_csv("data.csv")
df_genres=pd.read_csv("data_by_genres.csv")
df_year=pd.read_csv("data_by_year.csv")
df_artist=pd.read_csv("data_by_artist.csv")
df_w_genres=pd.read_csv("data_w_genres.csv")
df.head()

#info
df.info()

df_w_genres.info()

#check if any missing data
df.isna().sum()

#check if any missing data
df_w_genres.isna().sum()

df.describe()

df_w_genres.describe()

df1= pd.DataFrame(df['year'])
df1.value_counts()[0:25].to_dict()

"""Here, it can be seen that the data set called data.csv, which we are examining, contains tracks between 1920 and 2021. It can be observed that it contains approximately 1000 to 2000 different track information for each year. In this context, it would be correct to examine the distribution of specific characteristics in sub-groups by fewer years periods such as 10 years.

#Visualizations descriptive statistics of the dataset;


>  Visual explanations of features & sharing distributions

*  Shape of the dataset,
*  Data types of columns,
*  Number of missing values for each column

A measure of central tendency is a single value that attempts to describe a set of data by identifying the central position within that set of data.


*   Mean
*   Median
"""

df.hist(figsize=(20,20))
plt.show()

df_w_genres.hist(figsize=(20,20))
plt.show()

df.groupby(by='year')['danceability'].mean()

df.groupby(by='year')['danceability'].median().head()

df.groupby(by='year')['acousticness'].mean()

df.groupby(by='year')['acousticness'].median().head()

df.groupby(by='year')['duration_ms'].mean()

df.groupby(by='year')['duration_ms'].median().head()

df.groupby(by='year')['instrumentalness'].mean()

df.groupby(by='year')['instrumentalness'].median().head()

df.groupby(by='year')['liveness'].mean()

df.groupby(by='year')['liveness'].median().head()

df.groupby(by='year')['danceability'].mean().plot.barh(grid=True)

"""In order to make the examination more accurate, transactions can be made between 1990-2000 and 2000-2010."""

df1= df[df["year"] >= 1990]  # selection by boolean arrays
df2= df1[df1["year"] < 2000]
df3= df[df["year"] >= 2000]
df4= df3[df3["year"] < 2010]
df2.year.value_counts().to_dict()

#Change in the average value of danceability over a 20-year period
df1.groupby(by = 'year')['danceability'].mean().plot.barh(grid=True)

df4.year.value_counts().to_dict()

df2.groupby(by = 'year')['danceability'].mean().plot.barh(grid=True)

df4.groupby(by = 'year')['danceability'].mean().plot.barh(grid=True)

data = df4['danceability'].values
_, axes = plt.subplots(1,3,figsize=(15,5))
axes[0].hist(data, bins=4)
axes[1].hist(data, bins=8)
axes[2].hist(data, bins=10)
plt.title("danceability") 

plt.show()

data = df['acousticness'].values
_, axes = plt.subplots(1,3,figsize=(15,5))
axes[0].hist(data, bins=4)
axes[1].hist(data, bins=8)
axes[2].hist(data, bins=16)
plt.title("acousticness") 

plt.show()

fig, axes = plt.subplots(1, 2)
axes[0].set_title("Distribution of danceability")
axes[0].hist(df['danceability'], alpha=0.3, density=True)
axes[0].axvline(df['danceability'].mean(), 0, 1, c="r")

axes[1].set_title("Dist. of acousticness")
axes[1].hist(df['acousticness'], alpha=0.3, density=True)
axes[1].axvline(df['acousticness'].mean(), 0, 1, c="r")

plt.tight_layout()
plt.show()

""" In all 100 years of data, it can be observed that danceability of the tracks are distributed normally, while acoustşcness of the tracks U shaped distributed."""

fig, axes = plt.subplots(1, 2)
axes[0].set_title("Distribution of liveness")
axes[0].hist(df['liveness'], alpha=0.3, density=True)
axes[0].axvline(df['liveness'].mean(), 0, 1, c="r")
axes[1].set_title("Dist. of  acousticness")
axes[1].hist(df['acousticness'], alpha=0.3, density=True)
axes[1].axvline(df['acousticness'].mean(), 0, 1, c="r")

plt.tight_layout()
plt.show()

"""Although acousticness of the data U shaped distributed, the distribution of liveness of the data seems negatively skewed.  In that case, it can be argued that tracks with lower values of liveness in our dataset are causing the mean value of liveness to be lower than the majority of tracks reviews."""

fig, axes = plt.subplots(1, 2)
axes[0].set_title("Distribution of danceability")
axes[0].hist(df['danceability'], alpha=0.3, density=True)
axes[0].axvline(df['danceability'].mean(), 0, 1, c="r")
axes[1].set_title("Dist. of  speechiness")
axes[1].hist(df['speechiness'], alpha=0.3, density=True)
axes[1].axvline(df['speechiness'].mean(), 0, 1, c="r")

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(1, 2)
axes[0].set_title("Distribution of instrumentalness")
axes[0].hist(df['instrumentalness'], alpha=0.3, density=True)
axes[0].axvline(df['instrumentalness'].mean(), 0, 1, c="r")

axes[1].set_title("Dist. of  speechiness")
axes[1].hist(df['speechiness'], alpha=0.3, density=True)
axes[1].axvline(df['speechiness'].mean(), 0, 1, c="r")

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(1, 2)
axes[0].set_title("Distribution of danceability 90-20")
axes[0].hist(df2['danceability'], alpha=0.3, density=True)
axes[0].axvline(df2['danceability'].mean(), 0, 1, c="r")

axes[1].set_title("Dist. of acousticness 90-20")
axes[1].hist(df2['acousticness'], alpha=0.3, density=True)
axes[1].axvline(df2['acousticness'].mean(), 0, 1, c="r")

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(1, 2)
axes[0].set_title("Distribution of danceability 20-2010")
axes[0].hist(df4['danceability'], alpha=0.3, density=True)
axes[0].axvline(df4['danceability'].mean(), 0, 1, c="r")

axes[1].set_title("Dist. of acousticness 20-2010")
axes[1].hist(df4['acousticness'], alpha=0.3, density=True)
axes[1].axvline(df4['acousticness'].mean(), 0, 1, c="r")

plt.tight_layout()
plt.show()

"""#Analysis of the most popular artists and songs

### Top 15 artist according to their popularity *index*
"""

most15PopularArtist= df_artist.sort_values(by=['popularity']).tail(15)
most15PopularArtist

most15PopularArtist.sort_values(by=['popularity'])
plt.figure(1,figsize=(40,20))
label=most15PopularArtist['artists']
y=most15PopularArtist['popularity']
plt.bar(label,y)
plt.title("Top 15 artists according to their popularity index",fontsize=40)
plt.ylabel("# of popularity index",fontsize=40)
plt.xticks(range(len(label)),label,fontsize=40,rotation=90)
plt.bar(label,y,width = 0.5, color='turquoise')
plt.yticks(fontsize=40)
plt.show()

"""###Top 15 songs according to their popularity index


"""

most15PopularSongs= df.sort_values(by=['popularity']).tail(15)
most15PopularSongs

most15PopularSongs.sort_values(by=['popularity'])
plt.figure(1,figsize=(40,20))
label=most15PopularSongs['name']
y=most15PopularSongs['popularity']
plt.bar(label,y)
plt.title("Top 15 songs according to their popularity index",fontsize=40)
plt.ylabel("# of popularity index",fontsize=40)
plt.xticks(range(len(label)),label,fontsize=40,rotation=90)
plt.bar(label,y,width = 0.5, color='black')
plt.yticks(fontsize=40)
plt.show()

"""### Top 15 artist according to their songs count"""

mostCountedSongs=df_artist.sort_values(by=['count']).tail(15)
mostCountedSongs

mostCountedSongs.sort_values(by=['count'])
plt.figure(1,figsize=(40,20))
label=mostCountedSongs['artists']
y=mostCountedSongs['count']
plt.bar(label,y)
plt.title("Top 15 artists according to their songs count",fontsize=40)
plt.ylabel("# of popularity index",fontsize=40)
plt.xticks(range(len(label)),label,fontsize=40,rotation=90)
plt.bar(label,y,width = 0.5, color='purple')
plt.yticks(fontsize=40)
plt.show()

"""#Temporal analysis of features according to different genres

A new appropriate data frame was created by mapping the year informations and genre informations needed to perform temporal analysis on genres, datacsv and data_w_genres datasets to each other through artists.
"""

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
# %matplotlib inline
from ast import literal_eval
artist = df.copy()
artist['artists'] = artist['artists'].map(lambda x: literal_eval(x))
artist['artists'] = artist['artists'].map(lambda x: x[0])
df_w_genres=df_w_genres[['artists','genres']]
result = pd.merge(artist ,df_w_genres ,on="artists")
result = result.sort_values(by=['year'])
result.head()

"""After merging, it was observed that there was no genre information for some artists. These are missing value for the analysis that needs to be done. For this reason, it is necessary to remove it from the data so that the analysis can be realized."""

result=result[result.astype(str)['genres'] != '[]']
result.head()

"""
The list object has been converted to string to allow parent generations to be read and processed in sub-generes. In addition, rows containing multiple pieces of information about genres. The reason for this is that if the specific properties of the genres are wanted to be compared, it is appropriate to remove the rows whose properties are all the same.

"""

result['genres'].str
df5=result.copy()
df5['genres'] = df5['genres'].map(lambda x: literal_eval(x))
df5['genres'] = df5['genres'].map(lambda x: x[0])
df5['genres'].str;

df5.genres.value_counts().to_dict()

"""###Rock as parent genre"""

dataOfRock = df5[df5['genres'].str.contains("rock")] # rock as parent genre
dataOfRock.head()

dataOfRock.genres.value_counts().to_dict()

dataOfRock.hist(figsize=(20,20))
plt.show()

instrumentalnessrock = dataOfRock.groupby('year')['instrumentalness'].mean()
instrumentalnessrock = instrumentalnessrock.reset_index()
instrumentalnessrock.columns = ['year', 'mean']
plt.figure(1,figsize=(40,20))
label=instrumentalnessrock['year']
y=instrumentalnessrock['mean']
ax = instrumentalnessrock.plot.bar(x='year', y='mean', figsize=(20,10))

"""As can be seen above, the years in which the instrumentalness of the rock parent genre is dramatically high at 1920 and then 1949, while the value of this property usually varies between 0 and 0.3."""

danceability = dataOfRock.groupby('year')['danceability'].mean()
danceability = danceability.reset_index()
danceability.columns = ['year', 'mean']
plt.figure(1,figsize=(40,20))
label=danceability['year']
y=danceability['mean']
ax = danceability.plot.bar(x='year', y='mean', figsize=(20,10))

"""From 1961 on the general trend, it can be said that the danceability of the rock genre has fallen. However, the most notable outlier in the chart above is the year 1939 with the lowest danceability value."""

acousticness = dataOfRock.groupby('year')['acousticness'].mean()
acousticness = acousticness.reset_index()
acousticness.columns = ['year', 'mean']
plt.figure(1,figsize=(40,20))
label=acousticness['year']
y=acousticness['mean']
ax = acousticness.plot.bar(x='year', y='mean', figsize=(20,10))

"""Although the change in acousticness has been quite fluctuating over the years, it is possible to say that it has fluctuated in a narrower and lower range since 1964."""

plt.figure(figsize=(16, 10))
sns.set(style="whitegrid")
columns = ["acousticness","danceability","instrumentalness"]
for col in columns:
    x = dataOfRock.groupby("year")[col].mean()
    ax= sns.lineplot(x=x.index,y=x,label=col)
ax.set_title('Audio characteristics of songs for selected features over year')
ax.set_ylabel('mean')
ax.set_xlabel('Year')

"""The temporal changes of the characteristics of the species studied individually above are shown immediately above with a single graph. Thanks to this chart, it can be said that while the rock parent genre did not show stable acoustics, danceability or instrumentality before the 1960s, it has fluctuated in a more specific value range since the 1960s."""

plt.figure(figsize=(16, 10))
sns.set(style="whitegrid")
columns = ["speechiness","liveness","valence","energy"]
for col in columns:
    x = dataOfRock.groupby("year")[col].mean()
    ax= sns.lineplot(x=x.index,y=x,label=col)
ax.set_title('Audio characteristics of songs for selected features over year')
ax.set_ylabel('mean')
ax.set_xlabel('Year')

"""While it has been observed that the rock parent genre has remained almost stable at a fairly low value since the 1960s, it is possible to say that all four characteristics have fluctuated in much more specific narrow ranges since the 1960s.

###Rap as parent genre
"""

dataOfRap = df5[df5['genres'].str.contains("rap")]
dataOfRap

dataOfRap.genres.value_counts().to_dict()

instrumentalnessrap = dataOfRap.groupby('year')['instrumentalness'].mean()
instrumentalnessrap = instrumentalnessrap.reset_index()
instrumentalnessrap.columns = ['year', 'mean']
plt.figure(1,figsize=(40,20))
label=instrumentalnessrap['year']
y=instrumentalnessrap['mean']
ax = instrumentalnessrap.plot.bar(x='year', y='mean', figsize=(20,10))

"""As can be seen above, although the instrumentalness of the rap genre is generally quite low, it is quite high in 1978 and 1964 and 1955 respectively."""

danceabilityrap = dataOfRap.groupby('year')['danceability'].mean()
danceabilityrap = danceabilityrap.reset_index()
danceabilityrap.columns = ['year', 'mean']
plt.figure(1,figsize=(40,20))
label=danceabilityrap['year']
y=danceabilityrap['mean']
ax = danceabilityrap.plot.bar(x='year', y='mean', figsize=(20,10))

"""Although it has been seen that the rap genre has higher danceability values since 1988, in 1978 it can be observed that rap has the highest danceability value."""

acousticnessrap = dataOfRap.groupby('year')['acousticness'].mean()
acousticnessrap = acousticnessrap.reset_index()
acousticnessrap.columns = ['year', 'mean']
plt.figure(1,figsize=(40,20))
label=acousticnessrap['year']
y=acousticnessrap['mean']
ax = acousticnessrap.plot.bar(x='year', y='mean', figsize=(20,10))

"""It can be observed that the acoustics of the rap genre were significantly higher between 1920 and 1974 than in subsequent years."""

plt.figure(figsize=(16, 10))
sns.set(style="whitegrid")
columns = ["acousticness","danceability","instrumentalness"]
for col in columns:
    x = dataOfRap.groupby("year")[col].mean()
    ax= sns.lineplot(x=x.index,y=x,label=col)
ax.set_title('Audio characteristics of songs for selected features over year')
ax.set_ylabel('mean')
ax.set_xlabel('Year')

"""In the chart above, the change of three characteristics of the rap genre by year can be examined in a single chart. From here, it is clear that the features before and after 1970 are in two different and opposite trends to each other. For example, its acousticness were at a fairly high value and almost stable trend before the 1970s, while it fluctuated at a fairly low value and a narrow time interval from the 1970s."""

plt.figure(figsize=(16, 10))
sns.set(style="whitegrid")
columns = ["speechiness","liveness","valence","energy"]
for col in columns:
    x = dataOfRap.groupby("year")[col].mean()
    ax= sns.lineplot(x=x.index,y=x,label=col)
ax.set_title('Audio characteristics over year')
ax.set_ylabel('Measure')
ax.set_xlabel('Year')

"""It can be observed that the rap parent genre fluctuates considerably in values and narrow ranges much closer to each other after 1985.

#Comparison of features of sub-genres over time periods
"""

justRock = dataOfRock.drop(columns=['release_date','artists','id','key','mode','name','popularity','explicit'])
justRock=justRock[justRock['year']>=1960]
justRock=justRock[justRock['year']<=2000]
justRock.head()

"""Some columns that would not be included in the comparison were removed from the data, and a new dataframe was created with only rock songs from 1960-2000."""

justRock=justRock.groupby(['year', 'genres']).agg({'danceability':'mean','acousticness':'mean','energy':'mean','instrumentalness':'mean','liveness':'mean','loudness':'mean','tempo':'mean','valence':'mean'})
justRock = justRock.reset_index()
justRock['genres'].value_counts()[:25]

"""###Extracting the two of the subgenres of rock parent genre to examine them temporal changes of the features among 1960-2000:"""

cl_rock=justRock[justRock['genres']=='classic rock'] 
alb_rock=justRock[justRock['genres']=='album rock']

plt.figure(1,figsize=(40,20))
label=cl_rock['year']
y=cl_rock['acousticness'],cl_rock['danceability'],cl_rock['instrumentalness']
ax = cl_rock.plot.bar(x='year', y=['acousticness','danceability','instrumentalness'], figsize=(20,10))
ax.set_title('Songs Characteristics of classic Rock for Selected Over Time Period ',fontsize=20,color='red')

"""When we look at the graphic in this section, we see the changes of classic rock, a sub-genre of rock, such as acousticness, danceability and instrumentalness over the years. While the acousticness was high in the first years, it has declined over the years. Danceability has generally been at steady levels but has occasionally seen small increases. Finally, instrumentalness started to be preferred in the following years, although it could not be used at all in the first years.

"""

plt.figure(1,figsize=(20,10))
label=alb_rock['year']
y=alb_rock['acousticness'],alb_rock['danceability'],alb_rock['instrumentalness']
ax = alb_rock.plot.bar(x='year', y=['acousticness','danceability','instrumentalness'], figsize=(20,10))
ax.set_title('Songs Characteristics of Album Rock for Selected Over Time Period ',fontsize=20,color='red')

"""In this part, album rock, which is a sub-genre of another rock, has been examined. Danceability seems to be at the same level in every period. In the Instrumentalness part, we see that it always remains at low levels. While the acousticness feature was preferred more in the early days, there was a decrease in its use for the following periods.


"""

justRap = dataOfRap.drop(columns=['release_date','artists','id','key','mode','name','popularity','explicit'])
justRap=justRap[justRap['year']>=1980]
justRap=justRap[justRap['year']<=2020]
justRap.tail()

justRap=justRap.groupby(['year', 'genres']).agg({'danceability':'mean','acousticness':'mean','energy':'mean','instrumentalness':'mean','speechiness':'mean','loudness':'mean','tempo':'mean','duration_ms':'mean'})
justRap = justRap.reset_index()
justRap['genres'].value_counts()[:25]

"""###Extracting the two of the subgenres of rap parent genre to examine them temporal changes of the features among 1980-2000:"""

gangsta_rap=justRap[justRap['genres']=='gangster rap']
cali_rap=justRap[justRap['genres']=='cali rap']

plt.figure(1,figsize=(40,20))
label=gangsta_rap['year']
y=gangsta_rap['acousticness'],gangsta_rap['danceability'],gangsta_rap['speechiness'],gangsta_rap['energy']
ax = gangsta_rap.plot.bar(x='year', y=['acousticness','danceability','speechiness','energy'], figsize=(20,10))
ax.set_title('Songs Characteristics of gangster rap for Selected Over Time Period ',fontsize=20,color='red')

"""When we look at the graph in this section, we see the changes of ganster rap, a subgenre of rap, over the years, such as acousticness, danceability speechiness, and energy. We see that features such as enery and danceability are high in this genre, which was an expected feature for this genre. It seems that the acousticness feature is not preferred much. It seems that the speechiness feature has been stable over the years.

"""

plt.figure(1,figsize=(40,20))
label=cali_rap['year']
y=cali_rap['acousticness'],cali_rap['danceability'],cali_rap['speechiness'],cali_rap['energy']
ax = cali_rap.plot.bar(x='year', y=['acousticness','danceability','speechiness','energy'], figsize=(20,10))
ax.set_title('Songs Characteristics of cali rap for Selected Over Time Period ',fontsize=20,color='red')

"""When looking at the cali rap feature, it seems that the energy and danceability feature is at the top levels in the same way as the rap type and other rap types above. It appears to be at low levels of acousticness again and less preferred for rap. Speechiness, like the other rap genre above, can be seen at medium levels and has not changed much over the years.

# Our Future Goal

In the next parts we will try to find correlations with the popularity index according to features.this will be our first step.Our second goal is that according to the changing fashion of certain periods. We will be trying to find how the types of music changed in those periods. Our last goal is that the try to find statistical differences in sub-genres of one parent genres.
"""